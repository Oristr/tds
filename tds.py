# -*- coding: utf-8 -*-
"""tds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZJGpAVZZDyoe66G7pcvBvxoZGCcLs3nE

## imports
"""

#!pip install pyod
#!pip install metaod

import pyod
from pyod.utils.data import generate_data
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.ocsvm import OCSVM
from pyod.models.knn import KNN
from pyod.models.hbos import HBOS
from pyod.models.abod import ABOD
from pyod.models.loda import LODA
from pyod.models.cof import COF
from sklearn.metrics import average_precision_score

from metaod.models.utility import prepare_trained_model
from metaod.models.predict_metaod import select_model

# load pretrained MetaOD model
prepare_trained_model()
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import os
"""## utility functions to load datasets (thx to kaggle and pyod)
In our project we searched more datasets but here we used \\

1. credit-card dataset (Normal transactions: 56861, fraud transactions: 101)
2. loan dataset 
3. pulsar dataset (Non pulsar: 16259, Pulsars: 1639)
4. pyod generated dataset (We control the portion of anomalies because we generate this data)


pyod has feature to automatically generate datasets with outliers, and metaod was trained with that so we also wanted to include those datasets as well
"""

#ROOT_DIR = "C:\\Users\\orris\\OneDrive\\Desktop\\AnomalyDetection-main\\datasets\\"
ROOT_DIR = "datasets" + os.sep
def preprocess_creditcard(path = ROOT_DIR + "creditcard.csv"):
    data = pd.read_csv(path)[:25000]
    # data = data.head(10000)
    #print(data.shape)
    #print(data.head())
    data = data.drop(['Time'], axis=1)
    data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))
    X_train, X_test = train_test_split(data, test_size=0.2, random_state=0)
    y_train = X_train['Class']
    X_train = X_train.drop(['Class'], axis=1)
    y_test = X_test['Class']
    X_test = X_test.drop(['Class'], axis=1)
    X_train = X_train.values
    X_test = X_test.values
    X_train.shape
    return  X_train, y_train, X_test, y_test
  

def preprocess_loan(path = ROOT_DIR + 'loan_data.csv'):
    # Load data from the csv file
    df = pd.read_csv(path, index_col=None)

    # Change the dots in the column names to underscores
    df.columns = [c.replace(".", "_") for c in df.columns]
    #print(f"Number of rows/records: {df.shape[0]}")
    #print(f"Number of columns/variables: {df.shape[1]}")
    df.head()
    from sklearn.preprocessing import LabelEncoder
    features = df.columns[1:]
    x = df.loc[:, features]
    le = LabelEncoder()
    x.purpose = le.fit_transform(x.purpose)

    y = df.credit_policy
    from sklearn.model_selection import train_test_split
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
    return x_train, y_train ,x_test, y_test


def preprocess_pulsar(path = ROOT_DIR + "Pulsar.csv"):
    data = pd.read_csv(path)
    data.shape
    data = data.interpolate()
    train_y = data['Class']
    train_x = data.drop(['Class'], axis=1)
    return train_x, train_y, None, None # to follow our standard 4 tuple return



def using_pyod_data(n_train = 2000, n_test = 500, n_features = 5, contamination = 0.02):
    X_train, y_train, X_test, y_test = generate_data(
      n_train=n_train,
      n_test=n_test,
      n_features=n_features,
      contamination=contamination,
      random_state=42)
    return X_train, y_train, X_test, y_test


def load_data(dataset_name):
  if dataset_name == 'credit-card':
    return preprocess_creditcard("datasets//" + "creditcard.csv")
  if dataset_name == 'pulsars':
    return preprocess_loan("datasets//" + 'loan_data.csv')
  if dataset_name == 'loans':
    return preprocess_pulsar("datasets//" + 'Pulsar.csv')
  if dataset_name == 'pyod':
    return using_pyod_data()
  print('Error, invalid dataset name')
  return None, None, None, None


def create_model_from_line(line):
    la = line.split(' ')
    model_name = la[:1][0]
    model_args = la[1:]

    if len(model_args) == 2:
        model_args[0] = float(model_args[0].replace('(',"").replace(',',""))
        model_args[1] = model_args[1].replace(')',"")
    if len(model_args) == 1:
        model_args = int(model_args[0])

    print(model_name, model_args)
    model = None
    if model_name == 'LODA':
        model = LODA(n_bins=int(model_args[0]), n_random_cuts=int(model_args[1]))

    if model_name == 'ABOD':
        model = ABOD(n_neighbors = model_args)

    if model_name == 'Iforest':
        model = IForest(n_estimators=int(model_args[0]), max_features=int(model_args[1]))

    if model_name == 'kNN':
        model = KNN(n_neighbors=int(model_args[0]), method=model_args[1])

    if model_name == 'LOF':
        model = LOF(n_neighbors=int(model_args[0]), metric=model_args[1])

    if model_name == 'HBOS':
        model = HBOS(n_bins=int(model_args[0]), alpha=int(model_args[1]))

    if model_name == 'OCSVM':
        model = OCSVM(nu=model_args[0], kernel=model_args[1])

    if model_name == 'COF':
        model = COF(n_neighbors=model_args)
    return model


def get_models_from_line(line, jump_interval = 1):
    la = line.split(' ')
    model_name = la[:1][0]
    model_args = la[1:]

    if len(model_args) == 2:
        model_args[0] = float(model_args[0].replace('(',"").replace(',',""))
        model_args[1] = model_args[1].replace(')',"")
    if len(model_args) == 1:
        model_args = int(model_args[0])

    print(model_name, model_args)
    models = []
    if model_name == 'LODA':
        model = LODA(n_bins=int(model_args[0]), n_random_cuts=int(model_args[1]))
        for i in range(-2, 2, jump_interval):
          models.append(LODA(n_bins=int(model_args[0]) + i, n_random_cuts=int(model_args[1])))

    if model_name == 'ABOD':
        model = ABOD(n_neighbors = model_args)
        for i in range(-2, 2, jump_interval):
          models.append(ABOD(n_neighbors = model_args + i))

    if model_name == 'Iforest':
        model = IForest(n_estimators=int(model_args[0]), max_features=int(model_args[1]))
        for i in range(-2, 2, jump_interval):
          models.append(IForest(n_estimators=int(model_args[0]) + i, max_features=int(model_args[1])))

    if model_name == 'kNN':
        model = KNN(n_neighbors=int(model_args[0]), method=model_args[1])
        for i in range(-2, 2, jump_interval):
          models.append(KNN(n_neighbors=int(model_args[0]) + i, method=model_args[1]))

    if model_name == 'LOF':
        model = LOF(n_neighbors=int(model_args[0]), metric=model_args[1])
        for i in range(-2, 2, jump_interval):
          models.append(LOF(n_neighbors=int(model_args[0]) + i, metric=model_args[1]))

    if model_name == 'HBOS':
        model = HBOS(n_bins=int(model_args[0]), alpha=int(model_args[1]))
        for i in range(-2, 2, jump_interval):
          models.append(HBOS(n_bins=int(model_args[0]) + i, alpha=int(model_args[1])))

    if model_name == 'OCSVM':
        models.append(OCSVM(nu=model_args[0], kernel=model_args[1]))

    if model_name == 'COF':
        model = COF(n_neighbors=model_args)
        for i in range(-2, 2, jump_interval):
          models.append(COF(n_neighbors=model_args + i))

    return models

def run_model(model, X_train, y_train):
    score =  average_precision_score(y_train, model.fit(X_train).decision_scores_)
    return score


def run_model_from_line(line , X_train, y_train):
    model = create_model_from_line(line)
    score = run_model(model, X_train, y_train)
    print(line + " model Average Precision", score)
    return score

# run_model_from_line(selected_models[0])

"""#### detectors"""

def get_detectors(jump_range = 6):
    # randomness_flags = []
    BASE_ESTIMATORS = [
        LODA(n_bins=5, n_random_cuts=10),
        LODA(n_bins=5, n_random_cuts=20),
        LODA(n_bins=5, n_random_cuts=30),
        LODA(n_bins=5, n_random_cuts=40),
        LODA(n_bins=5, n_random_cuts=50),
        LODA(n_bins=5, n_random_cuts=75),
        LODA(n_bins=5, n_random_cuts=100),
        LODA(n_bins=5, n_random_cuts=150),
        LODA(n_bins=5, n_random_cuts=200),

        LODA(n_bins=10, n_random_cuts=10),
        LODA(n_bins=10, n_random_cuts=20),
        LODA(n_bins=10, n_random_cuts=30),
        LODA(n_bins=10, n_random_cuts=40),
        LODA(n_bins=10, n_random_cuts=50),
        LODA(n_bins=10, n_random_cuts=75),
        LODA(n_bins=10, n_random_cuts=100),
        LODA(n_bins=10, n_random_cuts=150),
        LODA(n_bins=10, n_random_cuts=200),

        LODA(n_bins=15, n_random_cuts=10),
        LODA(n_bins=15, n_random_cuts=20),
        LODA(n_bins=15, n_random_cuts=30),
        LODA(n_bins=15, n_random_cuts=40),
        LODA(n_bins=15, n_random_cuts=50),
        LODA(n_bins=15, n_random_cuts=75),
        LODA(n_bins=15, n_random_cuts=100),
        LODA(n_bins=15, n_random_cuts=150),
        LODA(n_bins=15, n_random_cuts=200),

        LODA(n_bins=20, n_random_cuts=10),
        LODA(n_bins=20, n_random_cuts=20),
        LODA(n_bins=20, n_random_cuts=30),
        LODA(n_bins=20, n_random_cuts=40),
        LODA(n_bins=20, n_random_cuts=50),
        LODA(n_bins=20, n_random_cuts=75),
        LODA(n_bins=20, n_random_cuts=100),
        LODA(n_bins=20, n_random_cuts=150),
        LODA(n_bins=20, n_random_cuts=200),

        LODA(n_bins=25, n_random_cuts=10),
        LODA(n_bins=25, n_random_cuts=20),
        LODA(n_bins=25, n_random_cuts=30),
        LODA(n_bins=25, n_random_cuts=40),
        LODA(n_bins=25, n_random_cuts=50),
        LODA(n_bins=25, n_random_cuts=75),
        LODA(n_bins=25, n_random_cuts=100),
        LODA(n_bins=25, n_random_cuts=150),
        LODA(n_bins=25, n_random_cuts=200),

        LODA(n_bins=30, n_random_cuts=10),
        LODA(n_bins=30, n_random_cuts=20),
        LODA(n_bins=30, n_random_cuts=75),
        LODA(n_bins=30, n_random_cuts=100),
        LODA(n_bins=30, n_random_cuts=150),
        LODA(n_bins=30, n_random_cuts=200),

        IForest(n_estimators=10, max_features=0.1),
        IForest(n_estimators=10, max_features=0.2),
        IForest(n_estimators=10, max_features=0.3),
        IForest(n_estimators=10, max_features=0.7),
        IForest(n_estimators=10, max_features=0.8),
        IForest(n_estimators=10, max_features=0.9),

        IForest(n_estimators=20, max_features=0.1),
        IForest(n_estimators=20, max_features=0.2),
        IForest(n_estimators=20, max_features=0.3),
        IForest(n_estimators=20, max_features=0.4),
        IForest(n_estimators=20, max_features=0.8),
        IForest(n_estimators=20, max_features=0.9),

        IForest(n_estimators=30, max_features=0.1),
        IForest(n_estimators=30, max_features=0.2),
        IForest(n_estimators=30, max_features=0.3),
        IForest(n_estimators=30, max_features=0.7),
        IForest(n_estimators=30, max_features=0.8),
        IForest(n_estimators=30, max_features=0.9),

        IForest(n_estimators=40, max_features=0.1),
        IForest(n_estimators=40, max_features=0.2),
        IForest(n_estimators=40, max_features=0.3),
        IForest(n_estimators=40, max_features=0.4),
        IForest(n_estimators=40, max_features=0.5),
        IForest(n_estimators=40, max_features=0.9),

        IForest(n_estimators=50, max_features=0.1),
        IForest(n_estimators=50, max_features=0.2),
        IForest(n_estimators=50, max_features=0.3),
        IForest(n_estimators=50, max_features=0.4),
        IForest(n_estimators=50, max_features=0.5),
        IForest(n_estimators=50, max_features=0.6),

        IForest(n_estimators=75, max_features=0.1),
        IForest(n_estimators=75, max_features=0.2),
        IForest(n_estimators=75, max_features=0.3),
        IForest(n_estimators=75, max_features=0.9),

        IForest(n_estimators=100, max_features=0.1),
        IForest(n_estimators=100, max_features=0.2),
        IForest(n_estimators=100, max_features=0.3),
        IForest(n_estimators=100, max_features=0.4),
        IForest(n_estimators=100, max_features=0.5),
        IForest(n_estimators=100, max_features=0.8),
        IForest(n_estimators=100, max_features=0.9),

        IForest(n_estimators=150, max_features=0.1),
        IForest(n_estimators=150, max_features=0.2),
        IForest(n_estimators=150, max_features=0.3),
        IForest(n_estimators=150, max_features=0.4),
        IForest(n_estimators=150, max_features=0.5),
        IForest(n_estimators=200, max_features=0.5),
        IForest(n_estimators=200, max_features=0.6),
        IForest(n_estimators=200, max_features=0.7),
        IForest(n_estimators=200, max_features=0.8),
        IForest(n_estimators=200, max_features=0.9),

        KNN(n_neighbors=1, method='largest'),
        KNN(n_neighbors=5, method='largest'),
        KNN(n_neighbors=10, method='largest'),
        KNN(n_neighbors=15, method='largest'),
        KNN(n_neighbors=20, method='largest'),
        KNN(n_neighbors=25, method='largest'),
        KNN(n_neighbors=50, method='largest'),
        KNN(n_neighbors=90, method='largest'),
        KNN(n_neighbors=100, method='largest'),

        KNN(n_neighbors=1, method='mean'),
        KNN(n_neighbors=5, method='mean'),
        KNN(n_neighbors=10, method='mean'),
        KNN(n_neighbors=15, method='mean'),
        KNN(n_neighbors=20, method='mean'),
        KNN(n_neighbors=25, method='mean'),
        KNN(n_neighbors=50, method='mean'),
        KNN(n_neighbors=100, method='mean'),

        KNN(n_neighbors=1, method='median'),
        KNN(n_neighbors=5, method='median'),
        KNN(n_neighbors=10, method='median'),
        KNN(n_neighbors=15, method='median'),
        KNN(n_neighbors=20, method='median'),
        KNN(n_neighbors=25, method='median'),
        KNN(n_neighbors=50, method='median'),
        KNN(n_neighbors=60, method='median'),
        KNN(n_neighbors=70, method='median'),
        KNN(n_neighbors=100, method='median'),

        LOF(n_neighbors=1, metric='manhattan'),
        LOF(n_neighbors=5, metric='manhattan'),
        LOF(n_neighbors=10, metric='manhattan'),
        LOF(n_neighbors=15, metric='manhattan'),
        LOF(n_neighbors=20, metric='manhattan'),
        LOF(n_neighbors=25, metric='manhattan'),
        LOF(n_neighbors=50, metric='manhattan'),
        LOF(n_neighbors=90, metric='manhattan'),
        LOF(n_neighbors=100, metric='manhattan'),

        LOF(n_neighbors=1, metric='euclidean'),
        LOF(n_neighbors=5, metric='euclidean'),
        LOF(n_neighbors=10, metric='euclidean'),
        LOF(n_neighbors=15, metric='euclidean'),
        LOF(n_neighbors=20, metric='euclidean'),
        LOF(n_neighbors=25, metric='euclidean'),
        LOF(n_neighbors=50, metric='euclidean'),
        LOF(n_neighbors=60, metric='euclidean'),
        LOF(n_neighbors=90, metric='euclidean'),
        LOF(n_neighbors=100, metric='euclidean'),

        LOF(n_neighbors=1, metric='minkowski'),
        LOF(n_neighbors=5, metric='minkowski'),
        LOF(n_neighbors=10, metric='minkowski'),
        LOF(n_neighbors=50, metric='minkowski'),
        LOF(n_neighbors=60, metric='minkowski'),
        LOF(n_neighbors=70, metric='minkowski'),
        LOF(n_neighbors=80, metric='minkowski'),
        LOF(n_neighbors=90, metric='minkowski'),
        LOF(n_neighbors=100, metric='minkowski'),

        HBOS(n_bins=5, alpha=0.1),
        HBOS(n_bins=5, alpha=0.2),
        HBOS(n_bins=5, alpha=0.3),
        HBOS(n_bins=5, alpha=0.4),
        HBOS(n_bins=5, alpha=0.5),

        HBOS(n_bins=10, alpha=0.1),
        HBOS(n_bins=10, alpha=0.2),
        HBOS(n_bins=10, alpha=0.3),
        HBOS(n_bins=10, alpha=0.4),
        HBOS(n_bins=10, alpha=0.5),

        HBOS(n_bins=20, alpha=0.1),
        HBOS(n_bins=20, alpha=0.2),
        HBOS(n_bins=20, alpha=0.3),
        HBOS(n_bins=20, alpha=0.4),
        HBOS(n_bins=20, alpha=0.5),

        HBOS(n_bins=30, alpha=0.1),
        HBOS(n_bins=30, alpha=0.2),
        HBOS(n_bins=30, alpha=0.3),
        HBOS(n_bins=30, alpha=0.4),
        HBOS(n_bins=30, alpha=0.5),

        HBOS(n_bins=40, alpha=0.1),
        HBOS(n_bins=40, alpha=0.2),
        HBOS(n_bins=40, alpha=0.3),
        HBOS(n_bins=40, alpha=0.4),
        HBOS(n_bins=40, alpha=0.5),

        HBOS(n_bins=50, alpha=0.1),
        HBOS(n_bins=50, alpha=0.2),
        HBOS(n_bins=50, alpha=0.3),
        HBOS(n_bins=50, alpha=0.4),
        HBOS(n_bins=50, alpha=0.5),

        HBOS(n_bins=75, alpha=0.1),
        HBOS(n_bins=75, alpha=0.2),
        HBOS(n_bins=75, alpha=0.3),
        HBOS(n_bins=75, alpha=0.4),
        HBOS(n_bins=75, alpha=0.5),

        HBOS(n_bins=100, alpha=0.1),
        HBOS(n_bins=100, alpha=0.2),
        HBOS(n_bins=100, alpha=0.3),
        HBOS(n_bins=100, alpha=0.4),
        HBOS(n_bins=100, alpha=0.5),

        OCSVM(nu=0.1, kernel="linear"),
        OCSVM(nu=0.2, kernel="linear"),
        OCSVM(nu=0.3, kernel="linear"),
        OCSVM(nu=0.4, kernel="linear"),
        OCSVM(nu=0.5, kernel="linear"),
        OCSVM(nu=0.6, kernel="linear"),
        OCSVM(nu=0.7, kernel="linear"),
        OCSVM(nu=0.8, kernel="linear"),
        OCSVM(nu=0.9, kernel="linear"),

        OCSVM(nu=0.1, kernel="poly"),
        OCSVM(nu=0.2, kernel="poly"),
        OCSVM(nu=0.3, kernel="poly"),
        OCSVM(nu=0.4, kernel="poly"),
        OCSVM(nu=0.5, kernel="poly"),
        OCSVM(nu=0.6, kernel="poly"),
        OCSVM(nu=0.7, kernel="poly"),
        OCSVM(nu=0.8, kernel="poly"),
        OCSVM(nu=0.9, kernel="poly"),

        OCSVM(nu=0.1, kernel="rbf"),
        OCSVM(nu=0.2, kernel="rbf"),
        OCSVM(nu=0.3, kernel="rbf"),
        OCSVM(nu=0.4, kernel="rbf"),
        OCSVM(nu=0.5, kernel="rbf"),
        OCSVM(nu=0.6, kernel="rbf"),
        OCSVM(nu=0.7, kernel="rbf"),
        OCSVM(nu=0.8, kernel="rbf"),
        OCSVM(nu=0.9, kernel="rbf"),

        OCSVM(nu=0.1, kernel="sigmoid"),
        OCSVM(nu=0.2, kernel="sigmoid"),
        OCSVM(nu=0.3, kernel="sigmoid"),
        OCSVM(nu=0.4, kernel="sigmoid"),
        OCSVM(nu=0.5, kernel="sigmoid"),
        OCSVM(nu=0.6, kernel="sigmoid"),
        OCSVM(nu=0.7, kernel="sigmoid"),
        OCSVM(nu=0.8, kernel="sigmoid"),
        OCSVM(nu=0.9, kernel="sigmoid"),

        COF(n_neighbors=3),
        COF(n_neighbors=5),
        COF(n_neighbors=10),
        COF(n_neighbors=15),
        COF(n_neighbors=20),
        COF(n_neighbors=25),
        COF(n_neighbors=50),
    ]

    return BASE_ESTIMATORS[::jump_range]


def run_all_detectors(X_train, y_train, detectors):
  scores_dict = {}
  for detector in detectors:
    #print("trying detector" + str(detector))
    try:
      scores_dict[str(detector)] = run_model(detector, X_train, y_train)
    except Exception:
      #print("exception in " + str(detector))
      scores_dict[str(detector)] = 0.0
  print(scores_dict)
  return scores_dict

def automate_model_selection(X, y, is_metaod_generated = False):
    #if not metaod Generated, selecting all detectors
    if is_metaod_generated == False:
      detectors = get_detectors()

    # if metaod generated, selecting the best metaod detectors
    if is_metaod_generated:
      selected_models = select_model(X, n_selection=20)
      print("This dataset is suitable for using metaOD! we will find the recommended models, and then try to improve them even more")
      best_model = selected_models[0]
      print("metaod recommended model is" + str(best_model))
      detectors = get_models_from_line(best_model)


    results = run_all_detectors(X, y, detectors)
    best_model = max(results, key=results.get)
    print("The best model is" + str(best_model))
    print("he got score of " + str(results[best_model]))
    return best_model, results


def plot_results_dict(best_model, results):
    fig = plt.figure(figsize=(20, 3))
    ax = fig.add_axes([0, 0, 1, 1])
    results_list = list(results.items())
    if len(results_list) > 5:
        results_list = list(results.items())[::14][:4]

    new_dict = dict(results_list)
    new_dict[best_model] = results[best_model]
    ax.bar(new_dict.keys(), new_dict.values(), align='edge', width=0.2)
    plt.show()
